# Pascal Aurele ELOUMOU

**Data and AI Solutions Analyst / Expert DATAIKU**  
10 années d'expérience  

---

## Compétences Majeures

### Langages  
- Python (Pandas, Numpy, Matplotlib, Scikit-learn, TensorFlow, PyTorch)  
- SQL (GoogleSQL, Snowflake SQL)  
- VBA  
- DAX / M (Power BI)  
- Bash / PowerShell  
- JavaScript (React Next.js)  
- R  

### Data Management et Outils  
- Dataiku  
- Snowflake  
- Tableau  
- Power BI  
- Looker  
- Alteryx  
- Traitement de données critiques (Banque, RH, RGPD)  

### Systèmes de gestion de bases de données  
- SQL Server  
- PostgreSQL  
- MongoDB (NoSQL)  

### Cloud  
- Google Cloud Platform (GCP)  
- Amazon Web Services (AWS)  
- Azure Data Factory  
- Talend  

### Méthodologies  
- Agile  

### Outils collaboratifs & DevOps  
- Jira  
- GitLab  
- Kubernetes  
- Docker  

### Excel  
- Excel avancé (Tableaux Croisés Dynamiques - TCD, Gestion de Codes de Gestion - GCD, VBA)  

### Dataiku  
- Maitrise des recipes : prepare, filter, join, window, formula, group  
- Dataiku DSS  
- Automatisation Machine Learning (AutoML)  
- Utilisation de modèles : Random Forest, Gradient  

### Certifications Dataiku  
- Dataiku Core Designer  
- Dataiku Advanced Core Designer  
- Dataiku Machine Learning Practitioner  
- Dataiku MLOps Practitioner  

---

## Formations, Diplômes et Certifications

- 2024-2025 : Master 2 Data & IA - HELTIC Paris  
- 2023-2024 : MBA1 Data Science - ESLCA Paris  
- 2010-2011 : Licence pro Système et Réseau - Institut Africain de l'Informatique  

---

## Langues

- Anglais : excellent niveau  

---

#Compétences comportementales
•	Compréhension fine des enjeux métiers pour aligner les actions aux objectifs stratégiques.
•	Communication claire, écoute active et adaptation au public et contexte.
•	Esprit critique permettant un diagnostic précis et des solutions pertinentes.
•	Travail collaboratif avec engagement, respect et contribution constructive.
•	Prise de décision informée en anticipant impacts et en évaluant les options.
•	Souplesse cognitive pour s’adapter aux changements et intégrer de nouvelles informations.
•	Sens de l’analyse structuré et capacité à interpréter les données pour formuler des conclusions.
•	Orientation résultats avec gestion des priorités selon contraintes de temps et ressources.
•	Maîtrise de la gestion de projet pour planifier, coordonner et assurer la qualité des livrables.
•	Curiosité intellectuelle et veille technologique pour rester à jour en data et IA.

Formations, diplômes et certifications
2024 – 2025	Master 2 Data & IA  - HETIC Paris 
2023 – 2024 	MBA1 Data Science – ESLCA Paris
2010 – 2011	Licence pro système et réseau – Institut africain de l’informatique
Langues
Anglais	★★★★★		


Expériences Professionnelles

Novembre 2024 - Actuellement – Manpower – Region parisienne
Fonction : Analyste et Data Engineer  
Contexte :
Projets stratégiques centrés sur la migration des plateformes Alteryx vers Snowflake, l’analyse prédictive, l’optimisation des budgets commerciaux et le ciblage client pour maximiser la performance des ventes.
Missions :
•	Mise en place de modèles prédictifs pour optimiser le ciblage marketing et les investissements commerciaux.
•	Développement de DataApps intégrant des modèles pré-entraînés avec finetuning pour contextualiser les analyses.
•	Analyse des corrélations entre enquêtes de satisfaction et indicateurs de performance d’agence pour améliorer la qualité de service.
•	Migration des processus de préparation, transformation et analyse de données d’Alteryx vers une architecture native Snowflake.
•	Gestion stricte des flux de données sensibles (données bancaires, RH, sécurité) avec conformité légale.
•	Exemple d’intégration NLP pour pondérer la notation quantitative NTSCORE avec l’analyse qualitative des verbatims.
•	Recueil des besoins métiers via ateliers, rédaction des exigences et optimisation des ressources.
•	Automatisation des flux Alteryx vers Snowflake pour données ultrasensibles.
•	Scripts Python et SQL pour automatiser l’injection et le traitement des données.
•	Connexion SFTP pour automatiser le workflow d’import CSV vers Snowflake et connexion Power BI.
•	Création de dashboards pour le contrôle des données salariés par les responsables d’agence.
•	Automatisation des KPI RH pour optimiser les workflows d’embauche et vérification.
•	Détection du sentiment et pondération pour obtenir une note globale qualitative et quantitative.
•	Analyse temporelle des KPI agence et recommandations automatiques basées sur ces KPI.
•	Interface Streamlit intégrée à Snowflake.
Résultats :
Réduction de 50% des malus liés aux fraudes.

Environnement technique : 
•	Environnement technique : Python, Snowflake, SQL,  Snowflake, Excel-VBA, Power BI, Alteryx

Septembre 2023 – Novembre 2024 – Carrefour Administratif France 

Fonction : Data Engineer / Data Analyst
Contexte : 
Optimisation des opérations fiscales à travers l’ETL et l’analyse des données 
Missions : 
•	Automatisation des pipelines de données en cloud, incluant l’orchestration via Cloud Functions pour bypasser la latence induite par la saturation de la bande passante SAP
•	Analyse des données d’écocontribution entrainant la réduction des risques de malus
•	Développement des automates pour le traitement automatique de fichiers Excel, réduisant une charge de travail équivalente à 6 personnes pendant 6 jours à seulement 4 h de traitement. 
•	Recherche de grains pour identifier uniquement les champs utiles aux requêtes métier. 
•	Stockage de ses données dans l'entrepôt de données de GCP dans le but d'accélérer les traitements. 
•	Intégration du BIG QUERY avec des requêtes SQL. 
Environnement technique : 
Python, SQL (Bigquery), Google Cloud Platform (GCP), Javascript, Looker studio (GDS), Jira. 


Septembre 2021 – Juillet 2023 – ENEO the Energy of Cameroun 
 
Fonction : Data Analyst
Contexte : Optimisation de la qualité de service en alignant les capacités RH avec les enjeux opérationnels via l’analyse CRM, stratégique et anti-fraude.
Missions : 
•	Analyse des données CRM pour mesurer l’impact de la production des capacités RH sur la qualité de service et la performance.
•	Croisement des données opérationnelles et stratégiques pour comparer les modèles pré-paiement vs post-paiement et réduire les pertes non-techniques tout en maximisant le chiffre d’affaires.
•	Détection de fraudes et pertes dignes d’attention grâce à l’analyse fine des données opérationnelles.
Environnement technique : 
Power BI, Python, CRM (Salesforce), Scrum & Jira, Excel (VBA, TCD, GCD)


Juillet 2015 - Juillet 2021 –Vinci Construction puis NGE 
 
Fonction : Data Analyst Expert 
Contexte : 
Optimisation des opérations de maintenance préventive des engins, des achats de matériel et de la gestion de flotte en intégrant des données embarquées terrain et des outils spécialisés pour la filiale SOGEA SATOM (Vinci Construction) 2015 - 2019
Puis recrutement par NGE pour réaliser des travaux similaires : 2019 - 2021
Pilotage stratégique de la flotte d’engins, avec un focus sur l’analyse des performances, la rentabilité et la budgétisation du service matériel. Conception des tableaux de bord · Suivi stratégique (KPI) · Reporting de performances · Gestion du matériel, des parcs de véhicules utilitaires, d’engins et autres équipements · Analyse des données

Missions :
•	Analyse des données opérationnelles des engins (utilisation, arrêts, coûts) pour évaluer rentabilité et efficacité.
•	Modélisation prédictive des budgets matériels pour anticiper dépenses, optimiser ressources et réduire coûts.
•	Création de rapports interactifs Power BI pour le suivi des KPI (cycles, coûts km/heure) destinés aux parties prenantes.
Développement workflows DATAIKU :
•	Automatisation de l’intégration des données OBD, CANBUS, météo (API OPENWEATHER), CSV et tickets SAP pour maintenance prédictive.
•	Extraction des données CANBUS via API REST et des logs GPS/anomalies CATERPILLAR via MyVisionLink, transfert FTP et centralisation sur DATAIKU DSS.
•	Reportings métiers adaptés aux équipes maintenance via DATAIKU.
Préparation des données :
•	Utilisation des recipes DATAIKU (prepare, filter, join) pour nettoyage, traitement et unification des données, garantissant leur qualité et pertinence.
•	Construction des KPI métiers via recipes (window, formula, group) sur DATAIKU.
•	Stockage et automatisation :
•	Données stockées dans bases PostgreSQL.
•	Automatisation de l’envoi d’alertes mails personnalisées selon seuils critiques (ex : risque panne 80%).
Machine learning :
•	Utilisation de modèles (Random Forest, Gradient Boosting) dans DATAIKU pour prédiction des pannes.
•	Auto-entrainement et évaluation des modèles avec accuracy et F1-score, choix du modèle optimal.
Résultats :
•	Disponibilité des engins : 81-92% en 6 mois.
•	Réduction du délai de détection des pannes de 5 à 1 jour.
•	12 pannes évitées mensuellement.
•	Réduction des heures perdues de 240 à 80 par mois.

Environnement technique : 
•	Python, Power BI, CRM (Salesforce), Scrum & Jira
•	SQL, Excel & Pivot Tables, MATIS, MHANET, CAT VisionLink, PAROS.


Fevrier 2013 – Aout 2014 – Kaysen Company
 
Fonction : Assistant responsable informatique 
Contexte : Installation complète du parc informatique : ordinateurs, serveurs, postes utilisateurs et périphériques. 
Prise en charge du support technique (helpdesk), résolution des incidents utilisateurs, gestion des tickets et assistance multimédias. 
Extension du réseau interne via fibre optique, garantissant une infrastructure robuste et performante.
Missions : 
•	Configuration et déploiement clé en main du matériel informatique sur site.
•	Mise en place d’un flux de tickets structuré pour un support réactif et efficace.
•	Déploiement de la connectivité fibre pour assurer une bande passante élevée et une excellente stabilité du réseau.
•	Matériel informatique & systèmes d’exploitation
•	Support helpdesk, gestion de tickets & résolution de problèmes
•	Infrastructure réseau fibre, câblage, gains de débit


Fevrier 2011 – Janvier 2013 – Groupement Andrade Gutierrez Zaggope
 
Fonction : Informaticien 
            Context : Installation et support du parc informatique
•	Installation complète du parc informatique : ordinateurs, serveurs, postes utilisateurs et périphériques.
•	Configuration et mise en service des équipements matériels et logiciels.
•	Prise en charge du support technique (helpdesk) : gestion et résolution des incidents utilisateurs.
•	Assistance multimédias et formation des utilisateurs.
•	Maintenance préventive et corrective des équipements.
. 




